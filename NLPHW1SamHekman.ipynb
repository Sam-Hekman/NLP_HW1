{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8390f8a7",
   "metadata": {},
   "source": [
    "# NLP Homework 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7e9232",
   "metadata": {},
   "source": [
    "Author: Sam Hekman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "71b1646c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries used\n",
    "import numpy as np\n",
    "import re\n",
    "import random # used for tie breaking in bigram text generation\n",
    "from math import log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0b2d73",
   "metadata": {},
   "source": [
    "## 1.\t[Points 10] Please write regular expressions for the following.                                         \n",
    "a.\tAll binary strings. Example binary strings, 1001, 1011, 1111, etc.\n",
    "b.\tThe email address contains only letters, and @, \\. Symbols (both lower and upper cases). Example:- alice@gmail.com, bob@yahoo.com, etc.\n",
    "c.\tValid integer numbers. Examples: 1, 12843, -89232, +1262, etc.\n",
    "d.\tValid phone number that contains ten (10) digits. Consider valid phone number formats are given below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0b4e3d",
   "metadata": {},
   "source": [
    "#### a. All binary strings. Example binary strings, 1001, 1011, 1111, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c39f3b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For part a, I am using the regex ^[0-1]+$ to include only 0s and 1s of variable length. The 0's and 1's must start the string,\n",
    "# this prevents the binary string from being matched as a substring of an otherwise invalid string\n",
    "\n",
    "binaryTestStrings = ['0', '1', '10', '11', '100', '101', '110', '111', '1000', '1001', '1010', '1011', '1100', '1101', '1110', '1111']\n",
    "nonBinaryTestStrings = ['2', '100101010109', '2001', '01810', 'notanumber', '&!*#&']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cd1f0153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 1), match='0'>\n",
      "<re.Match object; span=(0, 1), match='1'>\n",
      "<re.Match object; span=(0, 2), match='10'>\n",
      "<re.Match object; span=(0, 2), match='11'>\n",
      "<re.Match object; span=(0, 3), match='100'>\n",
      "<re.Match object; span=(0, 3), match='101'>\n",
      "<re.Match object; span=(0, 3), match='110'>\n",
      "<re.Match object; span=(0, 3), match='111'>\n",
      "<re.Match object; span=(0, 4), match='1000'>\n",
      "<re.Match object; span=(0, 4), match='1001'>\n",
      "<re.Match object; span=(0, 4), match='1010'>\n",
      "<re.Match object; span=(0, 4), match='1011'>\n",
      "<re.Match object; span=(0, 4), match='1100'>\n",
      "<re.Match object; span=(0, 4), match='1101'>\n",
      "<re.Match object; span=(0, 4), match='1110'>\n",
      "<re.Match object; span=(0, 4), match='1111'>\n"
     ]
    }
   ],
   "source": [
    "# Check valid binary strings\n",
    "for s in binaryTestStrings:\n",
    "    print(re.match('^[0-1]+$', s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "403953d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Check invalid binary strings\n",
    "for s in nonBinaryTestStrings:\n",
    "    print(re.match('^[0-1]+$', s))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4702da6",
   "metadata": {},
   "source": [
    "#### b. The email address contains only letters, and @, . Symbols (both lower and upper cases). Example:- alice@gmail.com, bob@yahoo.com, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c193e6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For part b, I am using ^[A-Za-z]+[@][A-Za-z]+[\\.][A-Za-z]+$ to match valid email addresses.\n",
    "# While this regex is long and unwieldly, it matches only strings that are email addresses as described in the question.\n",
    "# Numbers and non-alpabetical characters are not allowed, with exception of a single @ and \\. separated by alphabetical text\n",
    "# as well as being insensitive to case. The address must also not be a substring.\n",
    "\n",
    "validEmails = ['alice@gmail.com', 'bob@yahoo.com', 'ILIKECAPS@CAPS.NET', 'MiXeDCAsINg@AoL.CoM']\n",
    "invalidEmails = ['123@numbers.com', 'y_s@underscore.org', 'abcdefg', '1234567890', 'at@@gmail.com', 'ex@mark!.com', '@.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f95c001b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 15), match='alice@gmail.com'>\n",
      "<re.Match object; span=(0, 13), match='bob@yahoo.com'>\n",
      "<re.Match object; span=(0, 18), match='ILIKECAPS@CAPS.NET'>\n",
      "<re.Match object; span=(0, 19), match='MiXeDCAsINg@AoL.CoM'>\n"
     ]
    }
   ],
   "source": [
    "# Check valid addresses\n",
    "for s in validEmails:\n",
    "    print(re.match('^[A-Za-z]+[@][A-Za-z]+[\\.][A-Za-z]+$', s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a82a03b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Check invalid addresses\n",
    "for s in invalidEmails:\n",
    "    print(re.match('^[A-Za-z]+[@][A-Za-z]+[\\.][A-Za-z]+$', s))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0fd147",
   "metadata": {},
   "source": [
    "#### c. Valid integer numbers. Examples: 1, 12843, -89232, +1262, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "72f7e3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For part c, I am using the regex ^[-+]?[\\d]+$ that allows for only valid integers.\n",
    "# The integers must not be a substring and can optionally start with a single + or - symbol.\n",
    "validIntegers = ['1', '0', '734857', '+8829', '-9']\n",
    "otherNumbers = ['9.7', '1/2', 'abc', 'abc123', '+jkl', '-7+', '+']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d877aa9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 1), match='1'>\n",
      "<re.Match object; span=(0, 1), match='0'>\n",
      "<re.Match object; span=(0, 6), match='734857'>\n",
      "<re.Match object; span=(0, 5), match='+8829'>\n",
      "<re.Match object; span=(0, 2), match='-9'>\n"
     ]
    }
   ],
   "source": [
    "# Check valid ints\n",
    "for s in validIntegers:\n",
    "    print(re.match('^[-+]?[\\d]+$', s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b7fb9889",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Check invalid ints\n",
    "for s in otherNumbers:\n",
    "    print(re.match('^[-+]?[\\d]+$', s))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee48920b",
   "metadata": {},
   "source": [
    "#### d. Valid phone number that contains ten (10) digits. Consider valid phone number formats are given below.\n",
    "i.\txxx-xxx-xxxx<br>\n",
    "ii.\t(xxx) xxx-xxxx<br>\n",
    "Examples:<br>\n",
    "453-126-4570<br>\n",
    "(453) 126-4560"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9629c163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part d uses the regex [(]?[\\d]{3}[)]?[-)\\s]?[\\s]?[\\d]{3}[-\\s]?[\\d]{4}, which matches any NANP phone number using common formats\n",
    "# Does not allow for country code, NANP countries are all +1 anyway.\n",
    "validPhonenumbers = ['453-126-4570', '(453) 126-4560', '555 000 7890', '(123) 999 1234', '5557651221']\n",
    "invalidPhonenumbers = ['abc def ghij', '(9090) 000-7777', 'xxx-999-0000', '*&*^', '((404))_888_9797']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4249fb19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 12), match='453-126-4570'>\n",
      "<re.Match object; span=(0, 14), match='(453) 126-4560'>\n",
      "<re.Match object; span=(0, 12), match='555 000 7890'>\n",
      "<re.Match object; span=(0, 14), match='(123) 999 1234'>\n",
      "<re.Match object; span=(0, 10), match='5557651221'>\n"
     ]
    }
   ],
   "source": [
    "for s in validPhonenumbers:\n",
    "    print(re.match('[(]?[\\d]{3}[)]?[-\\s]?[\\s]?[\\d]{3}[-\\s]?[\\d]{4}', s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cf758e2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "for s in invalidPhonenumbers:\n",
    "    print(re.match('[(]?[\\d]{3}[)]?[-\\s]?[\\s]?[\\d]{3}[-\\s]?[\\d]{4}', s))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee1c9bf",
   "metadata": {},
   "source": [
    "## 2.\t[Points 5] Determine the number of tokens and vocabulary, and types from the below text. Please list them in your answer too. \n",
    "\n",
    "Text: “The quick brown fox jumps over the lazy dog.”"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262013c7",
   "metadata": {},
   "source": [
    "**Answer:** There are 9 tokens and 8 types in the vocabulary.<br>\n",
    "The tokens are: \"the\", \"quick\", \"brown\", \"fox\", \"jumps\", \"over\", \"the\", \"lazy\", \"dog\"<br>\n",
    "The types in the vocabulary are: 'brown', 'dog', 'fox', 'jumps', 'lazy', 'over', 'quick', 'the'<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894b99cc",
   "metadata": {},
   "source": [
    "## 3.\t[Points 5] Write down all the steps of text normalization and give an example for each step. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb06fcd",
   "metadata": {},
   "source": [
    "**Answer:** Text normalization consists of 3 steps: tokenization, Format normalizing, and sentence segmentation<br>\n",
    "<br>Tokenization is the segmentation of the text body of words into individual tokens. The delineation typically being spaces, at least in English. An example would be tokenizing \"I like Python\" into \"I\" and \"like\" and \"Python\".<br>\n",
    "<br>Format normalization can include text normalization tecniques such as lemmatization and stemming. In lemmatization, words are converted to a single common form. An example being \"laptops\", \"lap top\", and \"laptop's\" all become the single \"laptop\". Stemming functions in a similar way to lemmatization, reducing words to their root forms. An example would be to reduce \"wrote\", \"writing\", \"written\", and \"writes\" to a single root verb \"write\".<br>\n",
    "<br>Sentence segmentation determines the boundaries between sentences by looking for End of Sentence markers. This can be tricky as the full-stop character \".\" can be used in abbreviations or a decimal points, for example. A naive algorithm may erroneously decide that \"Let's get to Dr. Hazif's class.\" are two sentences, \"Let's get to Dr.\" and \"Hazif's class.\"<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1879edc7",
   "metadata": {},
   "source": [
    "## 4.\t[Points 15] We know how to compute similarity distance between two given strings using the edit distance algorithm. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d31fdd1",
   "metadata": {},
   "source": [
    "#### a.\t[Points 10] Please write down the distance matrix for the following strings. Consider space “ “ as a single character.\n",
    "String 1: **Spokesman confirms**\n",
    "<br>String 2: **Spokeswoman said**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0866a3",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "Assuming that deletions and insertions cost 1, while substitutions cost 2. The steps and costs are:<br>\n",
    "\n",
    "-> No change 'S', cost 0<br> \n",
    "-> No change 'P', cost 0<br>\n",
    "-> No change 'O', cost 0<br>\n",
    "-> No change 'K', cost 0<br>\n",
    "-> No change 'E', cost 0<br>\n",
    "-> No change 'S\", cost 0<br>\n",
    "-> Insert 'W', cost 1<br>\n",
    "-> Insert 'O', cost 2<br>\n",
    "-> No change 'M', cost 2<br>\n",
    "-> No change 'A', cost 2<br>\n",
    "-> No change 'N', cost 2<br>\n",
    "-> No change ' ', cost 2<br>\n",
    "-> Delete 'C', cost 3<br>\n",
    "-> Delete 'O', cost 4<br>\n",
    "-> Substitute 'N' for 'S', cost 6<br>\n",
    "-> Substitute 'F' for 'A', cost 8<br>\n",
    "-> No change 'I', cost 8<br>\n",
    "-> Delete 'R', cost 9<br>\n",
    "-> Delete 'M', cost 10<br>\n",
    "-> Substitute 'S' for 'D', cost 12<br>\n",
    "\n",
    "The cost of each step is show on the backtrace matrix in bold:\n",
    "\n",
    "|- | # | S | P | O | K | E | S | W | O | M | A | N | _ | S | A | I | D |\n",
    "|- | - | - | - | - | - | - | - | - | - | - | - | - | - | - | - | - | - |\n",
    "|**#** |**0**|1|2|3|4|5|6|7|8|9|10|11|12|13|14|15|16|\n",
    "|**S** |1|**0**|1|2|3|4|5|6|7|8|9|10|11|12|13|14|15|\n",
    "|**P** |2|1|**0**|1|2|3|4|5|6|7|8|9|10|11|12|13|14|\n",
    "|**O** |3|2|1|**0**|1|2|3|4|5|6|7|8|9|10|11|12|13|\n",
    "|**K** |4|3|2|1|**0**|1|2|3|4|5|6|7|8|9|10|11|12|\n",
    "|**E** |5|4|3|2|1|**0**|1|2|3|4|5|6|7|8|9|10|11|\n",
    "|**S** |6|5|4|3|2|1|**0**|**1**|**2**|3|4|5|6|7|8|9|10|\n",
    "|**M** |7|6|5|4|3|2|1|2|3|**2**|3|4|5|6|7|8|9|\n",
    "|**A** |8|7|6|5|4|3|2|3|4|3|**2**|3|4|5|6|7|8|\n",
    "|**N** |9|8|7|6|5|4|3|4|5|4|3|**2**|3|4|5|6|7|\n",
    "| _ |10|9|8|7|6|5|4|5|6|5|4|3|**2**|3|4|5|6|\n",
    "|**C** |11|10|9|8|7|6|5|6|7|6|5|4|**3**|4|5|6|7|\n",
    "|**O** |12|11|10|9|8|7|6|7|6|7|6|5|**4**|5|6|7|8|\n",
    "|**N** |13|12|11|10|9|8|7|8|7|8|7|6|5|**6**|7|8|9|\n",
    "|**F** |14|13|12|11|10|9|8|9|8|9|8|7|6|7|**8**|9|10|\n",
    "|**I** |15|14|13|12|11|10|9|10|9|10|9|8|7|8|9|**8**|9|\n",
    "|**R** |16|15|14|13|12|11|10|11|10|11|10|9|8|9|10|**9**|10|\n",
    "|**M** |17|16|15|14|13|12|11|12|11|10|11|10|9|10|11|**10**|11|\n",
    "|**S** |18|17|16|15|14|13|12|13|12|11|12|11|10|9|10|11|**12**|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b9cd15",
   "metadata": {},
   "source": [
    "## 5.\t[Points 20] Please formulate your language model for the following text. Show the details of your LM formulation. \n",
    "\n",
    "Text: “The day was grey and bitter cold, and the dogs would not take the scent. The big black bitch had taken one sniff at the bear tracks, backed off, and skulked back to the pack with her tail between her legs.”"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ccc090",
   "metadata": {},
   "source": [
    "#### a.\tUnigram model   [Points 10]<br>\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "A unigram language model is simply the combined probability of all words in a corpus such that:<br>\n",
    "P(w<sub>1</sub>, ...w<sub>n</sub>) = $\\prod$<sub>i</sub> w<sub>i</sub>\n",
    "\n",
    "This can also be described using log space as:<br>\n",
    "log(P(w<sub>1</sub>, ...w<sub>n</sub>)) = $\\sum$<sub>i</sub> log(w<sub>i</sub>)\n",
    "\n",
    "Optionally, laplacing smothing can be applied to ensure that the model does not fail on unseen tokens.\n",
    "\n",
    "As a result of being a unigram model, the probability of a word has no conditional probability based on previous sequences.\n",
    "\n",
    "**First, the text is tokenized and a vocabulary of types is created from those tokens.**\n",
    "\n",
    "\n",
    "Tokens = ['the', 'day', 'was', 'grey', 'and', 'bitter', 'cold', 'and', 'the', 'dogs', 'would', 'not', 'take', 'the', 'scent', 'the', 'big', 'black', 'bitch', 'had', 'taken', 'one', 'sniff', 'at', 'the', 'bear', 'tracks', 'backed', 'off', 'and', 'skulked', 'back', 'to', 'the', 'pack', 'with', 'her', 'tail', 'between', 'her', 'legs', ',', '.[end]', ',', ',', '.[end]']\n",
    "\n",
    "\n",
    "Types = [',', '.[end]', 'and', 'at', 'back', 'backed', 'bear', 'between', 'big',\n",
    "       'bitch', 'bitter', 'black', 'cold', 'day', 'dogs', 'grey', 'had',\n",
    "       'her', 'legs', 'not', 'off', 'one', 'pack', 'scent', 'skulked',\n",
    "       'sniff', 'tail', 'take', 'taken', 'the', 'to', 'tracks', 'was',\n",
    "       'with', 'would']\n",
    "\n",
    "**Next, the probabilities of each type is calculated**\n",
    "\n",
    "Using the example text, the language model can be described as:\n",
    "\n",
    "',', 3/46 = 0.06521739130434782<br>\n",
    "'.[end]', 2/46 = 0.043478260869565216<br>\n",
    "'and', 3/46 = 0.06521739130434782<br>\n",
    "'at', 1/46 = 0.021739130434782608<br>\n",
    "'back', 1/46 = 0.021739130434782608<br>\n",
    "'backed', 1/46 = 0.021739130434782608<br>\n",
    "'bear', 1/46 = 0.021739130434782608<br>\n",
    "'between', 1/46 = 0.021739130434782608<br>\n",
    "'big', 1/46 = 0.021739130434782608<br>\n",
    "'bitch', 1/46 = 0.021739130434782608<br>\n",
    "'bitter', 1/46 = 0.021739130434782608<br>\n",
    "'black', 1/46 = 0.021739130434782608<br>\n",
    "'cold', 1/46 = 0.021739130434782608<br>\n",
    "'day', 1/46 = 0.021739130434782608<br>\n",
    "'dogs', 1/46 = 0.021739130434782608<br>\n",
    "'grey', 1/46 = 0.021739130434782608<br>\n",
    "'had', 1/46 = 0.021739130434782608<br>\n",
    "'her', 2/46 = 0.043478260869565216<br>\n",
    "'legs', 1/46 = 0.021739130434782608<br>\n",
    "'not', 1/46 = 0.021739130434782608<br>\n",
    "'off', 1/46 = 0.021739130434782608<br>\n",
    "'one', 1/46 = 0.021739130434782608<br>\n",
    "'pack', 1/46 = 0.021739130434782608<br>\n",
    "'scent', 1/46 = 0.021739130434782608<br>\n",
    "'skulked', 1/46 = 0.021739130434782608<br>\n",
    "'sniff', 1/46 = 0.021739130434782608<br>\n",
    "'tail', 1/46 = 0.021739130434782608<br>\n",
    "'take', 1/46 = 0.021739130434782608<br>\n",
    "'taken', 1/46 = 0.021739130434782608<br>\n",
    "'the', 6/46 = 0.13043478260869565<br>\n",
    "'to', 1/46 = 0.021739130434782608<br>\n",
    "'tracks', 1/46 = 0.021739130434782608<br>\n",
    "'was', 1/46 = 0.021739130434782608<br>\n",
    "'with', 1/46 = 0.021739130434782608<br>\n",
    "'would', 1/46 = 0.021739130434782608<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbecc5ce",
   "metadata": {},
   "source": [
    "#### b.\tBigram model    [Points 10]<br>\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "A bigram language model is conditional probability of a word give it follows some other word such that:<br>\n",
    "P(w<sub>i</sub>|w<sub>1</sub>, ...w<sub>i-1</sub>) = P(w<sub>i</sub>|w<sub>i-1</sub>)\n",
    "\n",
    "\n",
    "**In addition to the tokenization and types seen from part a, a list of bigrams needs to be created.**\n",
    "\n",
    "bigrams: [',' 'and'], \n",
    " [',' 'backed'], \n",
    " ['[start]' 'the'], \n",
    " ['and' 'bitter'], \n",
    " ['and' 'skulked'], \n",
    " ['and' 'the'], \n",
    " ['at' 'the'], \n",
    " ['back' 'to'], \n",
    " ['backed' 'off'], \n",
    " ['bear' 'tracks'], \n",
    " ['between' 'her'], \n",
    " ['big' 'black'], \n",
    " ['bitch' 'had'], \n",
    " ['bitter' 'cold'], \n",
    " ['black' 'bitch'], \n",
    " ['cold' ','], \n",
    " ['day' 'was'], \n",
    " ['dogs' 'would'], \n",
    " ['grey' 'and'], \n",
    " ['had' 'taken'], \n",
    " ['her' 'legs'], \n",
    " ['her' 'tail'], \n",
    " ['legs' '[end]'], \n",
    " ['not' 'take'], \n",
    " ['off' ','], \n",
    " ['one' 'sniff'], \n",
    " ['pack' 'with'], \n",
    " ['scent' '[end]'], \n",
    " ['skulked' 'back'], \n",
    " ['sniff' 'at'], \n",
    " ['tail' 'between'], \n",
    " ['take' 'the'], \n",
    " ['taken' 'one'], \n",
    " ['the' 'bear'], \n",
    " ['the' 'big'], \n",
    " ['the' 'day'], \n",
    " ['the' 'dogs'], \n",
    " ['the' 'pack'], \n",
    " ['the' 'scent'], \n",
    " ['to' 'the'], \n",
    " ['tracks' ','], \n",
    " ['was' 'grey'], \n",
    " ['with' 'her'], \n",
    " ['would' 'not']\n",
    "\n",
    "**Next, the probabilities of each bigram is calculated**\n",
    "\n",
    "Using the example text, the language model can be described as:\n",
    "\n",
    "bigram: \", and\": 2/46 for probability of 0.043478260869565216<br>\n",
    "bigram: \", backed\": 1/46 for probability of 0.021739130434782608<br>\n",
    "bigram: \"[start] the\": 2/46 for probability of 0.043478260869565216<br>\n",
    "bigram: \"and bitter\": 1/46 for probability of 0.021739130434782608<br>\n",
    "bigram: \"and skulked\": 1/46 for probability of 0.021739130434782608<br>\n",
    "bigram: \"and the\": 1/46 for probability of 0.021739130434782608<br>\n",
    "bigram: \"at the\": 1/46 for probability of 0.021739130434782608<br>\n",
    "bigram: \"back to\": 1/46 for probability of 0.021739130434782608<br>\n",
    "bigram: \"backed off\": 1/46 for probability of 0.021739130434782608<br>\n",
    "bigram: \"bear tracks\": 1/46 for probability of 0.021739130434782608<br>\n",
    "bigram: \"between her\": 1/46 for probability of 0.021739130434782608<br>\n",
    "bigram: \"big black\": 1/46 for probability of 0.021739130434782608<br>\n",
    "bigram: \"bitch had\": 1/46 for probability of 0.021739130434782608<br>\n",
    "bigram: \"bitter cold\": 1/46 for probability of 0.021739130434782608<br>\n",
    "bigram: \"black bitch\": 1/46 for probability of 0.021739130434782608<br>\n",
    "bigram: \"cold ,\": 1/46 for probability of 0.021739130434782608<br>\n",
    "bigram: \"day was\": 1/46 for probability of 0.021739130434782608<br>\n",
    "bigram: \"dogs would\": 1/46 for probability of 0.021739130434782608<br>\n",
    "bigram: \"grey and\": 1/46 for probability of 0.021739130434782608<br>\n",
    "bigram: \"had taken\": 1/46 for probability of 0.021739130434782608<br>\n",
    "bigram: \"her legs\": 1/46 for probability of 0.021739130434782608<br>\n",
    "bigram: \"her tail\": 1/46 for probability of 0.021739130434782608<br>\n",
    "bigram: \"legs [end]\": 1/46 for probability of 0.021739130434782608<br>\n",
    "bigram: \"not take\": 1/46 for probability of 0.021739130434782608<br>\n",
    "bigram: \"off ,\": 1/46 for probability of 0.021739130434782608<br>\n",
    "bigram: \"one sniff\": 1/46 for probability of 0.021739130434782608<br>\n",
    "bigram: \"pack with\": 1/46 for probability of 0.021739130434782608<br>\n",
    "bigram: \"scent [end]\": 1/46 for probability of 0.021739130434782608<br>\n",
    "bigram: \"skulked back\": 1/46 for probability of 0.021739130434782608<br>\n",
    "bigram: \"sniff at\": 1/46 for probability of 0.021739130434782608<br>\n",
    "bigram: \"tail between\": 1/46 for probability of 0.021739130434782608<br>\n",
    "bigram: \"take the\": 1/46 for probability of 0.021739130434782608<br>\n",
    "bigram: \"taken one\": 1/46 for probability of 0.021739130434782608<br>\n",
    "bigram: \"the bear\": 1/46 for probability of 0.021739130434782608<br>\n",
    "bigram: \"the big\": 1/46 for probability of 0.021739130434782608<br>\n",
    "bigram: \"the day\": 1/46 for probability of 0.021739130434782608<br>\n",
    "bigram: \"the dogs\": 1/46 for probability of 0.021739130434782608<br>\n",
    "bigram: \"the pack\": 1/46 for probability of 0.021739130434782608<br>\n",
    "bigram: \"the scent\": 1/46 for probability of 0.021739130434782608<br>\n",
    "bigram: \"to the\": 1/46 for probability of 0.021739130434782608<br>\n",
    "bigram: \"tracks ,\": 1/46 for probability of 0.021739130434782608<br>\n",
    "bigram: \"was grey\": 1/46 for probability of 0.021739130434782608<br>\n",
    "bigram: \"with her\": 1/46 for probability of 0.021739130434782608<br>\n",
    "bigram: \"would not\": 1/46 for probability of 0.021739130434782608<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9bfa75",
   "metadata": {},
   "source": [
    "6.\t[Points 15] You are given a training set of 30 numbers that consists of 21 zeros and 1 each of the other digits 1-9. Now we see the following test set: 0 0 0 0 0 3 0 0 0 0. What is the unigram perplexity? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2c12ea",
   "metadata": {},
   "source": [
    "## 6.\t[Points 15] You are given a training set of 30 numbers that consists of 21 zeros and 1 each of the other digits 1-9. Now we see the following test set: 0 0 0 0 0 3 0 0 0 0. What is the unigram perplexity? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e48d76",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "Unigram perplexity can be shown as: PP(W) = P(w<sub>1</sub>, ...w<sub>n</sub>)<sup>-1/N\n",
    "    \n",
    "Given the training set, the unigram model can be described as:\n",
    "    \n",
    "'0', 21/30 = 0.7<br>\n",
    "'1', 1/30 = 0.03333333333333333<br>\n",
    "'2', 1/30 = 0.03333333333333333<br>\n",
    "'3', 1/30 = 0.03333333333333333<br>\n",
    "'4', 1/30 = 0.03333333333333333<br>\n",
    "'5', 1/30 = 0.03333333333333333<br>\n",
    "'6', 1/30 = 0.03333333333333333<br>\n",
    "'7', 1/30 = 0.03333333333333333<br>\n",
    "'8', 1/30 = 0.03333333333333333<br>\n",
    "'9', 1/30 = 0.03333333333333333<br>\n",
    "    \n",
    "The perplexity of the sample sentence \"0 0 0 0 0 3 0 0 0 0\" can be expressed as:\n",
    "    \n",
    "1.936974438134067 = PP(0 0 0 0 0 3 0 0 0 0) = P(0, 0, 0, 0, 0, 3, 0, 0, 0, 0)<sup>-1/10</sup>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd53b2c",
   "metadata": {},
   "source": [
    "## 7.\t[Points 15] Write a program to implement a minimum edit distance algorithm that takes two strings of any length and returns the total number of distances required to convert one string to another. Your program should be flexible to take flexible costs for the insertion, deletion, and substitute operations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0be4e09",
   "metadata": {},
   "source": [
    "**Min edit distance function:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cc155598",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MinEdit(string1, string2, subcost):\n",
    "    # Build backtrace matrix\n",
    "    backtrace = np.zeros((len(string1)+1, len(string2)+1)).astype(int)\n",
    "\n",
    "    # Get lengths of strings\n",
    "    stringlen1 = len(string1)\n",
    "    stringlen2 = len(string2)\n",
    "\n",
    "    # Build backtrace matrix\n",
    "    backtrace = np.zeros((stringlen1+1, stringlen2+1)).astype(int)\n",
    "    \n",
    "    # Set initial values\n",
    "    for i in range(stringlen1+1):\n",
    "        backtrace[i, 0] = i\n",
    "    for j in range(stringlen2+1):\n",
    "        backtrace[0, j] = j\n",
    "\n",
    "    # Calculate costs using algorithm from lecture slides\n",
    "    for i in range(stringlen1+1):\n",
    "        for j in range(stringlen2+1):\n",
    "            if i != 0 and j != 0:\n",
    "                deletion = backtrace[i, j-1] + 1\n",
    "                insertion = backtrace[i-1, j] + 1\n",
    "                if string1[i-1] != string2[j-1]:\n",
    "                    substitution = backtrace[i-1, j-1] + subcost\n",
    "                else:\n",
    "                    substitution = backtrace[i-1, j-1]\n",
    "\n",
    "                backtrace[i, j] = min(deletion, insertion, substitution)\n",
    "                \n",
    "    # Get minimum edit distance path and steps          \n",
    "    path = []\n",
    "    step = []\n",
    "    i = stringlen1\n",
    "    j = stringlen2\n",
    "    \n",
    "    # travel backwards to start, following min edit path\n",
    "    while i != 0:\n",
    "        path.insert(0, backtrace[i,j])\n",
    "        \n",
    "        # Case that there is no change made\n",
    "        if string1[i-1] == string2[j-1]:\n",
    "            step.insert(0, \"No change with \" + string1[i-1])\n",
    "            i -= 1\n",
    "            j -= 1\n",
    "        \n",
    "        # Case that edit was made, find type of edit made and traverse accordingly\n",
    "        else:\n",
    "            insertion = backtrace[i, j-1]\n",
    "            deletion = backtrace[i-1, j]\n",
    "            substitution = backtrace[i-1, j-1]\n",
    "            edit = min(deletion, insertion, substitution)\n",
    "            if edit == insertion:\n",
    "                step.insert(0, \"Insert \" + string2[j-1])\n",
    "                j -= 1\n",
    "            elif edit == deletion:\n",
    "                step.insert(0, \"Delete \" + string1[i-1])\n",
    "                i -= 1\n",
    "            elif edit == substitution:\n",
    "                step.insert(0, \"Substitute \" + string1[i-1] + \" for \" + string2[j-1])\n",
    "                i -= 1\n",
    "                j -= 1\n",
    "    \n",
    "    # Print outputs\n",
    "    \n",
    "    # Backtrace matrix\n",
    "    outmatrix = backtrace.astype(\"str\")\n",
    "    sl1 = list(string1)\n",
    "    sl1.insert(0, '#')\n",
    "    sl2 = list(string2)\n",
    "    sl2.insert(0, '#')\n",
    "    sl2.insert(0, \" \")\n",
    "    outmatrix = np.insert(outmatrix, 0, sl1, axis = 1)\n",
    "    outmatrix = np.insert(outmatrix, 0, sl2, axis = 0)\n",
    "    print(outmatrix)\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    # path and costs\n",
    "\n",
    "    for i in range(len(step)):\n",
    "        print(\"step {}: \".format(i+1) + step[i] + \"; Total cost = \" + path[i].astype(\"str\"))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "53396f31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[' ' '#' 'l' 'a' 'n' 'g' 'u' 'a' 'g' 'e']\n",
      " ['#' '0' '1' '2' '3' '4' '5' '6' '7' '8']\n",
      " ['n' '1' '2' '3' '2' '3' '4' '5' '6' '7']\n",
      " ['a' '2' '3' '2' '3' '4' '5' '4' '5' '6']\n",
      " ['t' '3' '4' '3' '4' '5' '6' '5' '6' '7']\n",
      " ['u' '4' '5' '4' '5' '6' '5' '6' '7' '8']\n",
      " ['r' '5' '6' '5' '6' '7' '6' '7' '8' '9']\n",
      " ['a' '6' '7' '6' '7' '8' '7' '6' '7' '8']\n",
      " ['l' '7' '6' '7' '8' '9' '8' '7' '8' '9']]\n",
      "\n",
      "\n",
      "step 1: Substitute n for l; Total cost = 2\n",
      "step 2: No change with a; Total cost = 2\n",
      "step 3: Insert n; Total cost = 3\n",
      "step 4: Substitute t for g; Total cost = 5\n",
      "step 5: No change with u; Total cost = 5\n",
      "step 6: Delete r; Total cost = 6\n",
      "step 7: No change with a; Total cost = 6\n",
      "step 8: Insert g; Total cost = 7\n",
      "step 9: Substitute l for e; Total cost = 9\n"
     ]
    }
   ],
   "source": [
    "# Driver code\n",
    "original_string = \"natural\"\n",
    "edited_string = \"language\"\n",
    "substitution_cost = 2\n",
    "\n",
    "MinEdit(original_string, edited_string, substitution_cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b220234",
   "metadata": {},
   "source": [
    "## 8.\t[Points 15] Write a program to construct bi-gram language model and provide source code to generate text using your bi-gram language model. Note: you could use any text dataset to construct your bi-gram language model. For simplicity I’m providing a short text example here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef97352f",
   "metadata": {},
   "source": [
    "#### Bigram generation code|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a20be382",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GenBigrams(text, length):\n",
    "    # Normalize cases\n",
    "    text = text.lower()\n",
    "\n",
    "    # Split into sentences\n",
    "    sentences = text.split('.')\n",
    "\n",
    "    # Remove last empty item resulting from splitting at the last full stop\n",
    "    sentences.remove('')\n",
    "\n",
    "    # Add start and end markers\n",
    "    for i in range(len(sentences)):\n",
    "        sentences[i] = \"[start] \" + sentences[i] + \" [end]\"\n",
    "    text2 = ' '.join(sentences)\n",
    "\n",
    "    # Tokenize the text\n",
    "    # I am counting \\. and . as the same character for the sake of simplicity.\n",
    "\n",
    "    words = text2.split(' ')\n",
    "    tokens = []\n",
    "\n",
    "    # Split alphanumeric strings and special characters like comma and period\n",
    "    for i in range(len(words)):\n",
    "\n",
    "        # Case that this string contains a special character\n",
    "        if re.sub('[^,\":;.]+', ' ', words[i]) != ' ':\n",
    "            # Split string into two parts\n",
    "            tokens.append(re.sub('[^A-Za-z0-9]+', '', words[i]))\n",
    "            tokens.append(re.sub('[^,\":;.]+', '', words[i]))\n",
    "        # Base case that string does not contain special character\n",
    "        else:\n",
    "            tokens.append(words[i])\n",
    "\n",
    "    # Clear tokens of accidental empty spaces\n",
    "    tokens[:] = (token for token in tokens if token != '')\n",
    "\n",
    "    # Get types and counts of types\n",
    "    types, counts = np.unique(tokens, return_counts=True)\n",
    "\n",
    "    # Build probability for types\n",
    "    probability = np.zeros(len(types))\n",
    "\n",
    "    for i in range(len(types)):\n",
    "        probability[i] = (counts[i] / len(tokens))\n",
    "\n",
    "    # Build dictionary for vocabulary with counts and probabilities\n",
    "    vocabulary = {}\n",
    "\n",
    "    for i in range(len(types)):\n",
    "        vocabulary[types[i]] = counts[i], probability[i]\n",
    "\n",
    "\n",
    "    # Build bigram model\n",
    "    all_bigrams = []\n",
    "    for i in range(len(tokens)-1):\n",
    "        all_bigrams.append([tokens[i], tokens[i+1]])\n",
    "    all_bigrams[:] = (bigram for bigram in all_bigrams if bigram != ['[end]', '[start]'])\n",
    "\n",
    "    # Get unique bigrams and their counts\n",
    "    bigrams, bigrams_counts = np.unique(all_bigrams, return_counts=True, axis = 0)\n",
    "        \n",
    "    length = length\n",
    "    new_words = ['[start]']\n",
    "\n",
    "    # compensate for start token\n",
    "    length = length + 1\n",
    "\n",
    "    for word in range(length):\n",
    "        candidates = []\n",
    "\n",
    "        # If start of new sentence\n",
    "        if new_words[word] == '[end]':\n",
    "            new_words.append('[start]')\n",
    "            # compensate for end and start tokens\n",
    "            length =  length + 2\n",
    "\n",
    "        else:\n",
    "            \n",
    "            # Find candidates based on probability|\n",
    "            for i in range(len(bigrams)):\n",
    "                if bigrams[i][0] == new_words[word] and bigrams[i][1] != new_words[word]:\n",
    "                    candidates.append(i)\n",
    "                    \n",
    "            # Case that there is only one candidate bigram\n",
    "            if len(candidates) == 1:\n",
    "                new_words.append(bigrams[candidates[0]][1])\n",
    "\n",
    "            # fail case\n",
    "            elif len(candidates) == 0:\n",
    "                rand = random(1, len(bigrams))\n",
    "                new_words.append(bigrams[rand][1])\n",
    "\n",
    "            # If tie\n",
    "            elif len(np.unique(candidates)) == 1:\n",
    "                rand = random.choice(candidates)\n",
    "                new_words.append(bigrams[rand][1])\n",
    "\n",
    "            # is not a tie\n",
    "            else:\n",
    "                top = np.max(candidates)\n",
    "                new_words.append(bigrams[top][1])\n",
    "                \n",
    "    out = ' '.join(new_words)\n",
    "    return(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5f116e1d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[start] you will see whether he may have lost the neighbours' respect , you will see whether he may have lost the\""
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Driver code\n",
    "# Testing using a few lines from the first chapter of 'The Hobbit'\n",
    "text = \"This hobbit was a very well to do hobbit, and his name was Baggins. The Bagginses had lived in the neighbourhood of The Hill for time out of mind, and people considered them very respectable, not only because most of them were rich, but also because they never had any adventures or did anything unexpected. You could tell what a Baggins would say on any question without the bother of asking him. This is a story of how a Baggins had an adventure, found himself doing and saying things altogether unexpected. He may have lost the neighbours' respect, but he gained well, you will see whether he gained anything in the end.\"\n",
    "length = 20\n",
    "\n",
    "GenBigrams(text, length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67522d6a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
